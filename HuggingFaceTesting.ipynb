{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My first Hugging Face test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing transformers, tokenizers, datasets packages in main Python environment\n",
    "\n",
    "I used this page to get going with Hugging Face and transformers. \n",
    "\n",
    "https://www.freecodecamp.org/news/get-started-with-hugging-face/\n",
    "\n",
    "So on my main Python installation I ran:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remember to use the main Python installation kernel at this point!\n",
    "%pip install transformers\n",
    "\n",
    "%pip install tokenizers, datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not sure if this is really necessary to do this on the main Python installation, since I'm later on is using a virtual environment where I installed Torch and I believe it automatically installed transformers and tokenizers in that virtual environment (see steps below).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a virtual Python environment\n",
    "\n",
    "I created the virtual environment as described on this page:\n",
    "\n",
    "https://docs.python.org/3/library/venv.html\n",
    "\n",
    "In a cmd window I ran the below to create the virtual environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "REM remeber to use the main Python installation at this point!\n",
    "python -m venv \"C:\\Temp\\PythonEnvironments\\HuggingFaceTest1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I ran the below in the same cmd window to activate the virtual environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "C:\\Temp\\PythonEnvironments\\HuggingFaceTest1\\Scripts\\activate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I ran the below in the same cmd window to install IPython Kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "pip install ipykernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I ran the below in the same cmd window to add the virtual environment as a kernel in Jupyter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "python -m ipykernel install --user --name=HuggingFaceTest1 --display-name=\"Python (HuggingFaceTest1)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, from this Jupyter notebook, you can switch the kernel by clicking on the kernel name (usually displayed at the top right of the notebook interface).  \n",
    "You should see the new kernel in the list of available kernels.\n",
    "\n",
    "**Note that a restart of the Jupyter notebook (or Visual Code) might be necessary to see the new kernel in the list of available kernels.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure we are using our new virtual environment we can run the below to see where pip is installed.  \n",
    "In my case I saw two entries for pip, one in the virtual environment and one in the main Python installation:  \n",
    "```console\n",
    "c:\\Temp\\PythonEnvironments\\HuggingFaceTest1\\Scripts\\pip.exe  \n",
    "C:\\Users\\dingvars\\AppData\\Local\\Programs\\Python\\Python312\\Scripts\\pip.exe\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we should switch to the new virtual Python environment kernel\n",
    "!where pip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also run the below to see which pip is being used.  \n",
    "It should be the one in the virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -V "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can the use the below to see which packages are installed in the virtual environment.  \n",
    "I did not see transformers or tokenizers packages installed in the virtual environment at this point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing transformers package in the virtual environment\n",
    "\n",
    "Then I installed Hugging Face Transformers in the virtual environment with the below command.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the below command I could see that transformers (and tokenizers) were now installed in the virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing misc packages in the virtual environment\n",
    "\n",
    "At this point I tried to run the test code below but received a number of error messages.  \n",
    "In the end I needed to run the below pip install commands to install misc packages to get the code to execute without errors.\n",
    "\n",
    "**After these packages are installed you must restart the virtual environment kernel to use updated packages.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch\n",
    "\n",
    "%pip install tensorflow\n",
    "\n",
    "%pip install ipywidgets\n",
    "\n",
    "%pip install tf-keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a model from Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we are ready for our first test of the huggingface transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load a pre-trained pipeline for sentiment analysis\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Use the pipeline\n",
    "result = sentiment_analysis(\"I love using Hugging Face Transformers!\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the models on the GPU\n",
    "\n",
    "I wanted to use the Nvidia RTX A2000 GPU that I have in my computer to run the models.\n",
    "\n",
    "To achive that I had to (re)install the Torch package with CUDA support.  \n",
    "https://en.wikipedia.org/wiki/CUDA  \n",
    "\n",
    "First I uninstalled the existing Torch package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall -y torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Then I (re)installed the Torch package with CUDA support.  \n",
    "I used this page to find the correct command:  \n",
    "https://pytorch.org/get-started/locally/\n",
    "\n",
    "**After these packages are installed you must restart the virtual environment kernel to use updated packages.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "#the cu118 part defines CUDA 11.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point I received errors when trying to execute the code below to verify the Torch installation.  \n",
    "To solve the issue I had to manually download and put the libomp140.x86_64.dll in the  \n",
    "C:\\Temp\\PythonEnvironments\\HuggingFaceTest1\\Lib\\site-packages\\torch\\lib folder.\n",
    "  \n",
    "I downloaded the dll from:  \n",
    "https://www.dllme.com/dll/files/libomp140_x86_64/00637fe34a6043031c9ae4c6cf0a891d/download\n",
    "\n",
    "**After doing this I had to restart the kernel for the code below to work.**\n",
    "\n",
    "The error message I received did not directly mention the libomp140.x86_64.dll file.  \n",
    "It looked something like:  \n",
    "\"*WinError 126, error loading fbgemm.dll or dependencies*\".  \n",
    "I used the Dependency tool to find out that it was actually the libomp140.x86_64.dll file that was missing.  \n",
    "https://github.com/lucasg/Dependencies\n",
    "\n",
    "After this I could run the code below to verify that Torch was installed correctly and that the GPU was available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__) # Should print the version of PyTorch you have installed\n",
    "print(torch.cuda.is_available())  # Should return True if CUDA is available and correctly set up\n",
    "print(torch.cuda.current_device())  # Should return 0 for the first GPU device\n",
    "print(torch.cuda.get_device_name(0))  # Should print the name of your GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code should now work OK, but in the output you should see that the GPU is not being used.  \n",
    "The message should be something like:  \n",
    "*\"Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "\n",
    "generator(\"In this course, we will teach you how to\", \n",
    "                   max_length=30, \n",
    "                   num_return_sequences=100, \n",
    "                   truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code does the same thing as the code above but here we specify that the first GPU device should be used for the model.  \n",
    "In my case this is my Nvidia RTX A2000 GPU.  \n",
    "This code also runs considerably faster than the code above since it uses the GPU for parallel processing.  \n",
    "The gain is larger the more sequences we process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Check if GPU is available and set the device accordingly\n",
    "device = 0 if torch.cuda.is_available() else -1  # 0 for GPU, -1 for CPU\n",
    "\n",
    "# Initialize the pipeline with the device argument\n",
    "generator = pipeline('text-generation', model='gpt2', device=device)\n",
    "\n",
    "# Call the generator with truncation=True\n",
    "generator(\"In this course, we will teach you how to\", \n",
    "                   max_length=30, \n",
    "                   num_return_sequences=100, \n",
    "                   truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding other models to play around with\n",
    "\n",
    "https://huggingface.co/models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
